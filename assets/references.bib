%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Manuel Gnthr at 2023-09-30 12:43:42 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{provost1997analysis,
	author = {Provost, Foster and Fawcett, Tom},
	booktitle = {Proceedings of AAAI-97 Workshop on AI Approaches to Fraud Detection \& Risk Management},
	date-added = {2023-09-30 12:43:30 +0200},
	date-modified = {2023-09-30 12:43:30 +0200},
	pages = {57--63},
	title = {Analysis and visualization of classifier performance with nonuniform class and cost distributions},
	year = {1997}}

@article{hanley1982meaning,
	author = {Hanley, James A and McNeil, Barbara J},
	date-added = {2023-09-29 15:59:57 +0200},
	date-modified = {2023-09-29 15:59:57 +0200},
	journal = {Radiology},
	number = {1},
	pages = {29--36},
	title = {The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
	volume = {143},
	year = {1982}}

@article{Gupta2018,
	abstract = {The functioning of electromyogram (EMG) driven prosthesis to control the performance of artificial prosthetic arms placed on people with missing limbs depends on the cumulative effect of multiple dynamic factors, some of which include electrode placement position, muscle contraction levels, forearm orientations, etc. However, the study of the combined influence of these dynamic factors has been limited and hence offered us scope to improve the accuracy of the previous studies. We used the data to extract multiple features through the Time Dependent Power Spectrum Descriptor (TD-PSD) algorithm, which has proven to be one of the best methods of feature extraction. Samples are classified using the Neural Pattern Recognition Toolbox with scaled conjugate gradient backpropagation as the training algorithm, which gives an improved accuracy over Support Vector Machine (SVM) classifier. Neural Network is trained using the EMG signals of 10 subjects performing multiple hand movements to achieve classification accuracy up to 94.7%. The results obtained are a testimony to the fact that the suggested method is competent to improve the operation of pattern recognition myoelectric signals.},
	author = {Gupta, Tanmay and Yadav, Jyoti and Chaudhary, Shubham and Agarwal, Utkarsh},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1007/978-3-319-68385-0_20},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/Borderline-SMOTE\: A New Over-Sampling Method in.pdf:pdf},
	isbn = {9783319683843},
	issn = {21945357},
	journal = {Advances in Intelligent Systems and Computing},
	keywords = {Classification,Clustering,Feature extraction,Pattern recognition},
	pages = {232--242},
	title = {{EMG pattern classification using neural networks}},
	volume = {683},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-68385-0_20}}

@article{Nguyen2009,
	author = {Nguyen, Hien M and Cooper, Eric W and Kamei, Katsuari},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/Borderline Over-sampling for Imbalanced Data.pdf:pdf},
	pages = {24--29},
	title = {{Borderline Over-sampling for Imbalanced Data Classification}},
	year = {2009}}

@article{OBrien2019,
	abstract = {Extending previous work on quantile classifiers (q-classifiers) we propose the q*-classifier for the class imbalance problem. The classifier assigns a sample to the minority class if the minority class conditional probability exceeds 0 < q* < 1, where q* equals the unconditional probability of observing a minority class sample. The motivation for q*-classification stems from a density-based approach and leads to the useful property that the q*-classifier maximizes the sum of the true positive and true negative rates. Moreover, because the procedure can be equivalently expressed as a cost-weighted Bayes classifier, it also minimizes weighted risk. Because of this dual optimization, the q*-classifier can achieve near zero risk in imbalance problems, while simultaneously optimizing true positive and true negative rates. We use random forests to apply q*-classification. This new method which we call RFQ is shown to outperform or is competitive with existing techniques with respect to G-mean performance and variable selection. Extensions to the multiclass imbalanced setting are also considered.},
	author = {O'Brien, Robert and Ishwaran, Hemant},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1016/j.patcog.2019.01.036},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/A random forests quantile classifier for class imbalanced data.pdf:pdf},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Class imbalance,Minority class,Random forests,Response-based sampling,Weighted Bayes classifier},
	pages = {232--249},
	publisher = {Elsevier Ltd},
	title = {{A random forests quantile classifier for class imbalanced data}},
	url = {https://doi.org/10.1016/j.patcog.2019.01.036},
	volume = {90},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.patcog.2019.01.036}}

@article{Tarawneh2022,
	abstract = {For the last two decades, oversampling has been employed to overcome the challenge of learning from imbalanced datasets. Many approaches to solving this challenge have been offered in the literature. Oversampling, on the other hand, is a concern. That is, models trained on fictitious data may fail spectacularly when put to real-world problems. The fundamental difficulty with oversampling approaches is that, given a real-life population, the synthesized samples may not truly belong to the minority class. As a result, training a classifier on these samples while pretending they represent minority may result in incorrect predictions when the model is used in the real world. We analyzed a large number of oversampling methods in this paper and devised a new oversampling evaluation system based on hiding a number of majority examples and comparing them to those generated by the oversampling process. Based on our evaluation system, we ranked all these methods based on their incorrectly generated examples for comparison. Our experiments using more than 70 oversampling methods and nine imbalanced real-world datasets reveal that all oversampling methods studied generate minority samples that are most likely to be majority. Given data and methods in hand, we argue that oversampling in its current forms and methodologies is unreliable for learning from class imbalanced data and should be avoided in real-world applications.},
	author = {Tarawneh, Ahmad S. and Hassanat, Ahmad B. and Altarawneh, Ghada Awad and Almuhaimeed, Abdullah},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1109/ACCESS.2022.3169512},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/Stop_Oversampling_for_Class_Imbalance_Learning_A_Review-2.pdf:pdf},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Hassanat metric,Oversampling,SMOTE,imbalanced datasets,machine learning},
	pages = {47643--47660},
	publisher = {IEEE},
	title = {{Stop Oversampling for Class Imbalance Learning: A Review}},
	volume = {10},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2022.3169512}}

@article{Xu2020,
	abstract = {The problem of imbalanced data classification often exists in medical diagnosis. Traditional classification algorithms usually assume that the number of samples in each class is similar and their misclassification cost during training is equal. However, the misclassification cost of patient samples is higher than that of healthy person samples. Therefore, how to increase the identification of patients without affecting the classification of healthy individuals is an urgent problem. In order to solve the problem of imbalanced data classification in medical diagnosis, we propose a hybrid sampling algorithm called RFMSE, which combines the Misclassification-oriented Synthetic minority over-sampling technique (M-SMOTE) and Edited nearset neighbor (ENN) based on Random forest (RF). The algorithm is mainly composed of three parts. First, M-SMOTE is used to increase the number of samples in the minority class, while the over-sampling rate of M-SMOTE is the misclassification rate of RF. Then, ENN is used to remove the noise ones from the majority samples. Finally, RF is used to perform classification prediction for the samples after hybrid sampling, and the stopping criterion for iterations is determined according to the changes of the classification index (i.e. Matthews Correlation Coefficient (MCC)). When the value of MCC continuously drops, the process of iterations will be stopped. Extensive experiments conducted on ten UCI datasets demonstrate that RFMSE can effectively solve the problem of imbalanced data classification. Compared with traditional algorithms, our method can improve F-value and MCC more effectively.},
	author = {Xu, Zhaozhao and Shen, Derong and Nie, Tiezheng and Kou, Yue},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1016/j.jbi.2020.103465},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/A hybrid sampling algorithm combining M-SMOTE and ENN based on.pdf:pdf},
	issn = {15320464},
	journal = {Journal of Biomedical Informatics},
	keywords = {Data resampling,Imbalanced data classification,Medical diagnosis,Random forest},
	number = {May 2019},
	pages = {103465},
	pmid = {32512209},
	publisher = {Elsevier},
	title = {{A hybrid sampling algorithm combining M-SMOTE and ENN based on Random forest for medical imbalanced data}},
	url = {https://doi.org/10.1016/j.jbi.2020.103465},
	volume = {107},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.jbi.2020.103465}}

@article{Khushi2021,
	abstract = {Medical datasets are usually imbalanced, where negative cases severely outnumber positive cases. Therefore, it is essential to deal with this data skew problem when training machine learning algorithms. This study uses two representative lung cancer datasets, PLCO and NLST, with imbalance ratios (the proportion of samples in the majority class to those in the minority class) of 24.7 and 25.0, respectively, to predict lung cancer incidence. This research uses the performance of 23 class imbalance methods (resampling and hybrid systems) with three classical classifiers (logistic regression, random forest, and LinearSVC) to identify the best imbalance techniques suitable for medical datasets. Resampling includes ten under-sampling methods (RUS, etc.), seven over-sampling methods (SMOTE, etc.), and two integrated sampling methods (SMOTEENN, SMOTE-Tomek). Hybrid systems include (Balanced Bagging, etc.). The results show that class imbalance learning can improve the classification ability of the model. Compared with other imbalanced techniques, under-sampling techniques have the highest standard deviation (SD), and over-sampling techniques have the lowest SD. Over-sampling is a stable method, and the AUC in the model is generally higher than in other ways. Using ROS, the random forest performs the best predictive ability and is more suitable for the lung cancer datasets used in this study. The code is available at https://mkhushi.github.io/},
	author = {Khushi, Matloob and Shaukat, Kamran and Alam, Talha Mahboob and Hameed, Ibrahim A. and Uddin, Shahadat and Luo, Suhuai and Yang, Xiaoyan and Reyes, Maranatha Consuelo},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1109/ACCESS.2021.3102399},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/A_Comparative_Performance_Analysis_of_Data_Resampling_Methods_on_Imbalance_Medical_Data.pdf:pdf},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Class imbalance,data resampling,healthcare,lung cancer,machine learning},
	pages = {109960--109975},
	title = {{A Comparative Performance Analysis of Data Resampling Methods on Imbalance Medical Data}},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2021.3102399}}

@article{Fotouhi2019,
	abstract = {The early diagnosis of cancer, as one of the major causes of death, is vital for cancerous patients. Diagnosing diseases in general and cancer in particular is a considerable application of data analysis for medical science. However, imbalanced data distribution and imbalanced quality of the majority and minority classes, which lead to misclassification, is a great challenge in this field. Though the samples of the majority class and their proper classification are more important to classifier, cancer is diagnosed by relying on the minority class samples (cancer data class). While the consequence of wrong diagnosis for non-cancerous patients is several additional clinical tests, the cancerous patients pay the price of wrong diagnosis with their lives. As such, studying the class imbalance problem is vital from the medical's perspective. To serve this purpose, a comprehensive study on the consequences of imbalanced data problem is performed in this paper on the data of cancer patients for the first time. In this context, oversampling and under sampling as two main balancing techniques including 18 algorithms are employed. The techniques used in oversampling are ADASYN, ADOMS, AHC, Borderline-SMOTE, ROS, Safe-Level-SMOTE, SMOTE, SMOTE-ENN, SMOTE-TL, SPIDER and SPIDER2, while under sampling techniques are CNN, CNNTL, NCL, OSS, RUS, SBC and TL. To examine the impact of balancers on the performance of classifiers, four classifiers named RIPPER, MLP, KNN, and C4.5 are employed as learners. In addition, 15 cancer data sets from SEER program used for the study are kidney, soft tissue, bladder, rectum, colon, bone, larynx, breast, cervix, prostate, oropharynx, melanoma, thyroid, testis, and lip. The findings of the study are centered on examining the impact of class imbalance on the function of classifiers, a general comparing of the function of pre-processing techniques and classifying all data sets and finally determining the best balancer and classifier for each kind of cancer data set. According to the results, significant improvement is obtained through using balancers. Assessing by AUC, the performance of different classifiers of cancer imbalanced data sets has improved in 90% of the cases after using balancing techniques. To be more precise, Friedman statistical tests are applied and interestingly, each kind of cancer data set responded differently to different balancing techniques and classifiers. Moreover, considering the mean rank of each technique and classifier that were used for data sets, oversampling balancing techniques result in better outcomes than under sampling ones.},
	author = {Fotouhi, Sara and Asadi, Shahrokh and Kattan, Michael W.},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1016/j.jbi.2018.12.003},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/1-s2.0-S1532046418302302-main.pdf:pdf},
	issn = {15320464},
	journal = {Journal of Biomedical Informatics},
	keywords = {Classification,Data pre-processing,Diagnosis of cancer,Imbalanced data},
	number = {December 2018},
	pages = {103089},
	pmid = {30611011},
	publisher = {Elsevier},
	title = {{A comprehensive data level analysis for cancer diagnosis on imbalanced data}},
	url = {https://doi.org/10.1016/j.jbi.2018.12.003},
	volume = {90},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.jbi.2018.12.003}}

@article{Brandt2020,
	abstract = {In this thesis, the performance of two over-sampling techniques, SMOTE and ADASYN, is compared. The comparison is done on three imbalanced data sets using three different classification models and evaluation metrics, while varying the way the data is pre-processed. The results show that both SMOTE and ADASYN improve the performance of the classifiers in most cases. It is also found that SVM in conjunction with SMOTE performs better than with ADASYN as the degree of class imbalance increases. Furthermore, both SMOTE and ADASYN increase the relative performance of the Random forest as the degree of class imbalance grows. However, no pre-processing method consistently outperforms the other in its contribution to better performance as the degree of class imbalance varies.},
	author = {Brandt, Jakob and Lanz{\'{e}}n, Emil},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/A Comparative Review of SMOTE.pdf:pdf},
	keywords = {ADASYN,F-measure,Machine learning,Matthews correlation coefficient 3,SMOTE,Sensitivity,class imbalance,classification,over-sampling,supervised learning},
	pages = {42},
	title = {{A Comparative Review of SMOTE and ADASYN in Imbalanced Data Classification}},
	year = {2020}}

@article{He2008,
	abstract = {This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics. {\textcopyright}2008 IEEE.},
	author = {He, Haibo and Bai, Yang and Garcia, Edwardo A. and Li, Shutao},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1109/IJCNN.2008.4633969},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/He, Garcia, Li - ADASYN_ Adaptive Synthetic Sampling Approach for Imbalanced Learning (2008) - 2008-He-ieee 2.pdf:pdf},
	isbn = {9781424418213},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	number = {3},
	pages = {1322--1328},
	title = {{ADASYN: Adaptive synthetic sampling approach for imbalanced learning}},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN.2008.4633969}}

@InProceedings{Han2005,
author="Han, Hui
and Wang, Wen-Yuan
and Mao, Bing-Huan",
editor="Huang, De-Shuang
and Zhang, Xiao-Ping
and Huang, Guang-Bin",
title="Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning",
booktitle="Advances in Intelligent Computing",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="878--887",
abstract="In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique (SMOTE) is one of the over-sampling methods addressing this problem. Based on SMOTE method, this paper presents two new minority over-sampling methods, borderline-SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods.",
isbn="978-3-540-31902-3"
}


@article{Liu2022,
	abstract = {Background: Imbalance between positive and negative outcomes, a so-called class imbalance, is a problem generally found in medical data. Despite various studies, class imbalance has always been a difficult issue. The main objective of this study was to find an effective integrated approach to address the problems posed by class imbalance and to validate the method in an early screening model for a rare cardiovascular disease aortic dissection (AD). Methods: Different data-level methods, cost-sensitive learning, and the bagging method were combined to solve the problem of low sensitivity caused by the imbalance of two classes of data. First, feature selection was applied to select the most relevant features using statistical analysis, including significance test and logistic regression. Then, we assigned two different misclassification cost values for two classes, constructed weak classifiers based on the support vector machine (SVM) model, and integrated the weak classifiers with undersampling and bagging methods to build the final strong classifier. Due to the rarity of AD, the data imbalance was particularly prominent. Therefore, we applied our method to the construction of an early screening model for AD disease. Clinical data of 523,213 patients from the Institute of Hypertension, Xiangya Hospital, Central South University were used to verify the validity of this method. In these data, the sample ratio of AD patients to non-AD patients was 1:65, and each sample contained 71 features. Results: The proposed ensemble model achieved the highest sensitivity of 82.8%, with training time and specificity reaching 56.4 s and 71.9% respectively. Additionally, it obtained a small variance of sensitivity of 19.58 × 10--3 in the seven-fold cross validation experiment. The results outperformed the common ensemble algorithms of AdaBoost, EasyEnsemble, and Random Forest (RF) as well as the single machine learning (ML) methods of logistic regression, decision tree, k nearest neighbors (KNN), back propagation neural network (BP) and SVM. Among the five single ML algorithms, the SVM model after cost-sensitive learning method performed best with a sensitivity of 79.5% and a specificity of 73.4%. Conclusions: In this study, we demonstrate that the integration of feature selection, undersampling, cost-sensitive learning and bagging methods can overcome the challenge of class imbalance in a medical dataset and develop a practical screening model for AD, which could lead to a decision support for screening for AD at an early stage.},
	author = {Liu, Lijue and Wu, Xiaoyu and Li, Shihao and Li, Yi and Tan, Shiyang and Bai, Yongping},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1186/s12911-022-01821-w},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/Solving the class imbalance problem using.pdf:pdf},
	issn = {14726947},
	journal = {BMC Medical Informatics and Decision Making},
	keywords = {Aortic dissection,Class imbalance,Ensemble learning,SVM},
	number = {1},
	pages = {1--16},
	pmid = {35346181},
	title = {{Solving the class imbalance problem using ensemble algorithm: application of screening for aortic dissection}},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1186/s12911-022-01821-w}}

@article{Tarawneh2020,
	abstract = {Class imbalance occurs in classification problems in which the 'normal' cases, or instances, significantly outnumber the 'abnormal' instances. Training a standard classifier on imbalanced data leads to predictive biases which cause poor performance on the class(es) with lower prior probabilities. The less frequent classes are often critically important events, such as system failure or the occurrence of a rare disease. As a result, the class imbalance problem has been considered to be of great importance for many years. In this paper, we propose a novel algorithm that utilizes the furthest neighbor of a candidate example to generate new synthetic samples. A key advantage of SOMTEFUNA over existing methods is that it does not have parameters to tune (such as K in SMOTE). Thus, it is significantly easier to utilize in real-world applications. We evaluate the benefit of resampling with SOMTEFUNA against state-of-the-art methods including SMOTE, ADASYN and SWIM using Naive Bayes and Support Vector Machine classifiers. Also, we provide a statistical analysis based on Wilcoxon Signed-rank test to validate the significance of the SMOTEFUNA results. The results indicate that the proposed method is an efficient alternative to the current methods. Specifically, SOMTEFUNA achieves better 5-fold cross validated ROC and precision-recall space performance.},
	author = {Tarawneh, Ahmad S. and Hassanat, Ahmad B.A. and Almohammadi, Khalid and Chetverikov, Dmitry and Bellinger, Colin},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1109/ACCESS.2020.2983003},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/SMOTEFUNA.pdf:pdf},
	isbn = {1620170000},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Binary classification,SMOTE,data mining algorithm,furthest neighbor,imbalance problem},
	number = {April},
	pages = {59069--59082},
	title = {{SMOTEFUNA: Synthetic Minority Over-Sampling Technique Based on Furthest Neighbour Algorithm}},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2020.2983003}}

@article{Chawla2002,
	abstract = {An approach to the construction of classifiers from unbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy. {\textcopyright} 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
	archiveprefix = {arXiv},
	arxivid = {1106.1813},
	author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1613/jair.953},
	eprint = {1106.1813},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/SMOTE\: Synthetic Minority Over-sampling Technique.pdf:pdf},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	pages = {321--357},
	title = {{SMOTE: Synthetic minority over-sampling technique}},
	volume = {16},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1613/jair.953}}

@article{Krawczyk2016,
	abstract = {Despite more than two decades of continuous development learning from imbalanced data is still a focus of intense research. Starting as a problem of skewed distributions of binary tasks, this topic evolved way beyond this conception. With the expansion of machine learning and data mining, combined with the arrival of big data era, we have gained a deeper insight into the nature of imbalanced learning, while at the same time facing new emerging challenges. Data-level and algorithm-level methods are constantly being improved and hybrid approaches gain increasing popularity. Recent trends focus on analyzing not only the disproportion between classes, but also other difficulties embedded in the nature of data. New real-life problems motivate researchers to focus on computationally efficient, adaptive and real-time methods. This paper aims at discussing open issues and challenges that need to be addressed to further develop the field of imbalanced learning. Seven vital areas of research in this topic are identified, covering the full spectrum of learning from imbalanced data: classification, regression, clustering, data streams, big data analytics and applications, e.g., in social media and computer vision. This paper provides a discussion and suggestions concerning lines of future research for each of them.},
	author = {Krawczyk, Bartosz},
	date-added = {2023-09-29 14:45:19 +0200},
	date-modified = {2023-09-29 14:45:19 +0200},
	doi = {10.1007/s13748-016-0094-0},
	file = {:home/zekiye/Documents/Master_lectures/Team Project/Learning from imbalanced data\: open challenges.pdf:pdf},
	issn = {21926360},
	journal = {Progress in Artificial Intelligence},
	keywords = {Big data,Data streams,Imbalanced clustering,Imbalanced data,Imbalanced regression,Machine learning,Multi-class imbalance},
	number = {4},
	pages = {221--232},
	publisher = {Springer Berlin Heidelberg},
	title = {{Learning from imbalanced data: open challenges and future directions}},
	volume = {5},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s13748-016-0094-0}}

@article{VanCalster2015,
	abstract = { Decision-analytic measures to assess clinical utility of prediction models and diagnostic tests incorporate the relative clinical consequences of true and false positives without the need for external information such as monetary costs. Net Benefit is a commonly used metric that weights the relative consequences in terms of the risk threshold at which a patient would opt for treatment. Theoretical results demonstrate that clinical utility is affected by a model';s calibration, the extent to which estimated risks correspond to observed event rates. We analyzed the effects of different types of miscalibration on Net Benefit and investigated whether and under what circumstances miscalibration can make a model clinically harmful. Clinical harm is defined as a lower Net Benefit compared with classifying all patients as positive or negative by default. We used simulated data to investigate the effect of overestimation, underestimation, overfitting (estimated risks too extreme), and underfitting (estimated risks too close to baseline risk) on Net Benefit for different choices of the risk threshold. In accordance with theory, we observed that miscalibration always reduced Net Benefit. Harm was sometimes observed when models underestimated risk at a threshold below the event rate (as in underestimation and overfitting) or overestimated risk at a threshold above event rate (as in overestimation and overfitting). Underfitting never resulted in a harmful model. The impact of miscalibration decreased with increasing discrimination. Net Benefit was less sensitive to miscalibration for risk thresholds close to the event rate than for other thresholds. We illustrate these findings with examples from the literature and with a case study on testicular cancer diagnosis. Our findings strengthen the importance of obtaining calibrated risk models. },
	author = {Ben Van Calster and Andrew J. Vickers},
	date-added = {2023-09-28 20:29:19 +0200},
	date-modified = {2023-09-29 14:46:01 +0200},
	doi = {10.1177/0272989X14547233},
	eprint = {https://doi.org/10.1177/0272989X14547233},
	journal = {Medical Decision Making},
	note = {PMID: 25155798},
	number = {2},
	pages = {162-169},
	title = {Calibration of Risk Prediction Models: Impact on Decision-Analytic Performance},
	url = {https://doi.org/10.1177/0272989X14547233},
	volume = {35},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1177/0272989X14547233}}

@article{harm_imbalance,
	abstract = {{Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models.Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: no correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented.The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: the probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead.Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed.Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.}},
	author = {van den Goorbergh, Ruben and van Smeden, Maarten and Timmerman, Dirk and Van Calster, Ben},
	date-added = {2023-09-28 20:22:31 +0200},
	date-modified = {2023-09-28 21:06:52 +0200},
	doi = {10.1093/jamia/ocac093},
	eprint = {https://academic.oup.com/jamia/article-pdf/29/9/1525/45444435/ocac093.pdf},
	issn = {1527-974X},
	journal = {Journal of the American Medical Informatics Association},
	month = {06},
	number = {9},
	pages = {1525-1534},
	title = {{The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression}},
	url = {https://doi.org/10.1093/jamia/ocac093},
	volume = {29},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1093/jamia/ocac093}}
