\subsection{Classifiers}
%describe each classifier and the class

\textbf{Logistic regression} is a commonly used statistical method for classification tasks. However, it assumes an equal distribution of classes, which may not hold true for imbalanced datasets. To address this, a class weighting mechanism is applied, which adjusts how the algorithm learns from different samples. This ensures that the model pays more attention to the minority class while being more lenient with errors from the majority class.
In practice, logistic regression proves to be a straightforward and efficient tool for binary and linear classification. It performs well when classes can be easily separated linearly. This method is widely employed due to its simplicity and practicality. Scikit-learn offers an optimized implementation of logistic regression, including support for multiclass classification tasks.

\textbf{Decision trees} are widely used in machine learning due to their interpretability. They make hierarchical decisions based on features, starting with all training examples at the root node. The tree grows by partitioning data at each node based on the best feature. While individual trees may not generalize well, ensembles of trees can significantly improve classification performance. However, decision trees perform best with balanced class distributions and struggle with imbalanced data, as they aim for optimal separation of samples.

\textbf{Random forest} is a versatile ensemble learning method suitable for classification, regression, and various other tasks. It constructs multiple decision trees during training. In classification, the final class is determined by majority vote, while for regression, it considers the average prediction. This method addresses overfitting tendencies in decision trees. While generally outperforming standalone trees, it may have slightly lower accuracy compared to gradient-boosted trees, influenced by dataset characteristics.
One key feature is the introduction of randomness in tree-building. Instead of searching for the most critical feature, it evaluates a random subset. This leads to a diverse and robust model. In a random forest, only a random subset of features is considered when splitting a node. Further randomness can be added by using random thresholds for each feature.
Overall, random forests effectively overcome limitations of single decision trees, reducing overfitting and enhancing accuracy. They operate seamlessly without requiring extensive configurations, making them valuable in both regression and classification tasks.