\subsection{Data balancing}

A data balancer attempts to close the gap between the number of samples in the majority class and the ones in the minority class.
This can be done by undersampling the majority class, oversampling the minority class or creating artificial samples for the minority class.
There is a fundamental problem apparent when thinking of how the latter can be accomplished:
If we knew the characteristics that distinguish the minority class from the majority class so well, that we could reliably create samples from it,
then the entire classification step would in a way be redundant as we could already assert class membership by those characteristics.
For this reason, SMOTE and other synthetic sampling methods can only produce imperfect representations of the real minority distributions.

With this in mind here are some important characteristics that we would hope a useful balancer can fulfill:
\begin{enumerate}[label=$\bullet$]
\item The balancer should be generalisable and applicable to different levels of imbalance and subsequent classification methods;
\item It should maintain the actual class structure and represent the minority class pattern accurately;
\item It should support a correct class identification.
\end{enumerate}
   
We hereby summarise the main features of the data-level balancing algorithms that we implemented in the pipeline. Typically, these methods are categorised into three main groups: synthetic samplers, resamplers and hybrid samplers.

\subsubsection{SMOTE}
\textbf{SMOTE} (Synthetic Minority Oversampling Technique) is probably the most known of the balancing methods. 
SMOTE is an over-sampling technique which expands the training set $\mathcal{D}_\text{train}$ by generating synthetic minority class samples.

Given the parameter $k$, at every iteration the algorithm does the following steps:

\begin{enumerate}[label=(\roman*)]
\item Select a random sample $(x_j^1, y_j^1)$ of the minority class
\item For the feature vector $x_j^1$ find the $k$ nearest minority neighbours $x_{l_1}^1 \dots x_{l_k}^1$
\item Select one of these neighbours at random, say $x_{l}^1$ and sample a uniform $u \sim \mathcal{U}(0,1)$
\item Add $(x, 1)$ with $x = x_j^1 + u (x_j^1 - x_l^1)$ to the training set
\end{enumerate}

until the desired number of minority samples is achieved.
This process creates new data points along line segments between minority feature vectors, effectively making the decision boundary of the minority class more inclusive. 
We decided to include SMOTE in our study as it is a reference method, even though important drawbacks have been detected in previous articles, 
such as presenting computational complexity quadratic in the size of the minority class and distortion of its distribution due to localisation of the selected target points.



\begin{comment}
	\begin{figure}[H]
	\label{fig:tube}
	\centering
	\includegraphics[height=4.0cm]{tube.jpg}
	\caption{Conservation in thin long tube (3D), with variables varying only in one dimension (from C.P. Fall, 2005)}
	\end{figure}
\end{comment}


\subsubsection{Borderline SMOTE}

Borderline-SMOTE is a specialized technique designed to handle imbalanced datasets. It focuses on oversampling examples near class boundaries, which are crucial for accurate classification. Unlike traditional oversampling methods that generate synthetic examples for the entire minority class, Borderline-SMOTE specifically targets these borderline examples within the minority class.

This method determines if a minority class instance is eligible for oversampling with the SMOTE technique based on the majority class representation among its nearest neighbors. By doing so, Borderline-SMOTE ensures a more precise and effective oversampling strategy.

Furthermore, implementing specific variants like Borderline-SMOTE I or Borderline-SMOTE II can be advantageous, especially in cases with outliers or sparsely scattered minority observations. These variants provide an extra layer of refinement to the oversampling process, enhancing the algorithm's performance.

In essence, Borderline-SMOTE's targeted approach, coupled with its careful consideration of instance eligibility, equips it to strengthen the learning process, particularly in regions near class boundaries where accurate classification is most critical. By incorporating this technique, imbalanced datasets can undergo a tailored oversampling treatment, ultimately improving the model's ability to navigate complex class boundaries~\cite{Nguyen2009,Gupta2018,Brandt2020}.


%\subsubsection{K-means SMOTE}


\subsubsection{ADASYN} is another synthetic sampler 
The ADASYN algorithm addresses class imbalance by generating synthetic observations tailored to the learning difficulty of specific minority class instances. It focuses on producing more synthetic data for minority observations that pose a greater challenge for the model to learn. This approach is similar to SMOTE, as it also creates synthetic observations for the minority class. However, ADASYN distinguishes itself by prioritizing the generation of synthetic data for instances that are comparatively harder to learn within the given model.

It generates additional synthetic observations for minority class instances located in regions where there are more majority class observations within the k-nearest neighbors' range. Conversely, if a minority observation has no majority class neighbors within this range, no synthetic data is generated for that particular instance. The rationale behind this lies in the understanding that instances without nearby majority class samples are inherently more challenging to learn than those situated further from majority class observations.

Fundamentally, ADASYN's approach is rooted in the adaptive generation of minority data samples, tailoring the synthesis process to the distribution and learning difficulty of the minority class~\cite{Brandt2020, He2008}.









\begin{comment}
% I put this algorithm here for reference on how algorithm2e is used
\NoCaptionOfAlgo
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlCapSkip{1em}
\SetAlCapNameFnt{\normalfont\normalsize}
%\TitleOfAlgo{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}
\caption{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}

Select a random sample of the minority class\;

\For{c in \texttt{cells}}{
    $
    \textcolor{darkgray}{r_c} :=
    \begin{cases*}
        \color{teal}{r_S} &\text{if c is sensitive} \\
        \color{purple}{r_R} &\text{if c is resistent}\\
    \end{cases*}$\;
    Total propensity $\textcolor{darkgray}{p := (r_c + d_T) \delta t}$\;
    \uIf{$\textcolor{Purple}{u < p}$}{ \tcp{cell active}
        \uIf{$\textcolor{Purple}{u < \frac{r_c \delta t}{p}}$}{
            \texttt{AttemptProliferation}\;
        }
        \Else{
            \texttt{Death}\;
        }
    } 
    \Else{  \tcp{cell inactive}
        \texttt{continue}\;
    }
}
\end{algorithm}
\end{comment}
