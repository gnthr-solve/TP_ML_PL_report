\subsection{Data balancing}
As for the data balancing step, here are some important characteristics that determine the usefulness of a balancer:
\begin{enumerate}[label=$\bullet$]
\item The balancer should be generalisable and applicable to different levels of imbalance and subsequent classification methods;
\item It should maintain the actual class structure and represent the minority class pattern accurately;
\item It should support a correct class identification.
\end{enumerate}
   
We hereby summarise the main features of the data-level balancing algorithms that we implemented in the pipeline. Typically, these methods are categorised into three main groups: synthetic samplers, resamplers and hybrid samplers.

\subsubsection{SMOTE}
\textbf{SMOTE} (Synthetic Minority Oversampling Technique) is probably the most known of the balancing methods. 
SMOTE is an over-sampling technique which expands the training set $\mathcal{D}_\text{train}$ by generating synthetic minority class samples.

Given the parameter $k$, at every iteration the algorithm does the following steps:

\begin{enumerate}[label=(\roman*)]
\item Select a random sample $(x_j^1, y_j^1)$ of the minority class
\item For the feature vector $x_j^1$ find the $k$ nearest minority neighbours $x_{l_1}^1 \dots x_{l_k}^1$
\item Select one of these neighbours at random, say $x_{l}^1$ and sample a uniform $u \sim \mathcal{U}(0,1)$
\item Add $(x, 1)$ with $x = x_j^1 + u (x_j^1 - x_l^1)$ to the training set
\end{enumerate}

until the desired number of minority samples is achieved.
This process creates new data points along line segments between minority feature vectors, effectively making the decision boundary of the minority class more inclusive. 
We decided to include SMOTE in our study as it is a reference method, even though important drawbacks have been detected in previous articles, 
such as presenting computational complexity quadratic in the size of the minority class and distortion of its distribution due to localisation of the selected target points.



\begin{comment}
	\begin{figure}[H]
	\label{fig:tube}
	\centering
	\includegraphics[height=4.0cm]{tube.jpg}
	\caption{Conservation in thin long tube (3D), with variables varying only in one dimension (from C.P. Fall, 2005)}
	\end{figure}
\end{comment}


\subsubsection{Borderline SMOTE}


%\subsubsection{K-means SMOTE}


\textbf{ADASYN} is another synthetic sampler 










\begin{comment}
% I put this algorithm here for reference on how algorithm2e is used
\NoCaptionOfAlgo
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlCapSkip{1em}
\SetAlCapNameFnt{\normalfont\normalsize}
%\TitleOfAlgo{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}
\caption{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}

Select a random sample of the minority class\;

\For{c in \texttt{cells}}{
    $
    \textcolor{darkgray}{r_c} :=
    \begin{cases*}
        \color{teal}{r_S} &\text{if c is sensitive} \\
        \color{purple}{r_R} &\text{if c is resistent}\\
    \end{cases*}$\;
    Total propensity $\textcolor{darkgray}{p := (r_c + d_T) \delta t}$\;
    \uIf{$\textcolor{Purple}{u < p}$}{ \tcp{cell active}
        \uIf{$\textcolor{Purple}{u < \frac{r_c \delta t}{p}}$}{
            \texttt{AttemptProliferation}\;
        }
        \Else{
            \texttt{Death}\;
        }
    } 
    \Else{  \tcp{cell inactive}
        \texttt{continue}\;
    }
}
\end{algorithm}
\end{comment}
