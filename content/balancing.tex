\subsection{Data balancing}
As for the data balancing step, here are some important characteristics that determine the usefulness of a balancer:
\begin{enumerate}[label=$\bullet$]
\item The balancer should be generalisable and applicable to different levels of imbalance and subsequent classification methods;
\item It should maintain the actual class structure and represent the minority class pattern accurately;
\item It should support a correct class identification.
\end{enumerate}
   
We hereby summarise the main features of the data-level balancing algorithms that we implemented in the pipeline. Typically, these methods are categorised into three main groups: synthetic samplers, resamplers and hybrid samplers.

\subsubsection{SMOTE}
\textbf{SMOTE} (Synthetic Minority Oversampling Technique) is probably the most known of the balancing methods. 
SMOTE is an over-sampling technique which expands the training set $\mathcal{D}_\text{train}$ by generating synthetic minority class samples.

Given the parameter $k$, at every iteration the algorithm does the following steps:

\begin{enumerate}[label=(\roman*)]
\item Select a random sample $(x_j^1, y_j^1)$ of the minority class
\item For the feature vector $x_j^1$ find the $k$ nearest minority neighbours $x_{l_1}^1 \dots x_{l_k}^1$
\item Select one of these neighbours at random, say $x_{l}^1$ and sample a uniform $u \sim \mathcal{U}(0,1)$
\item Add $(x, 1)$ with $x = x_j^1 + u (x_j^1 - x_l^1)$ to the training set
\end{enumerate}

until the desired number of minority samples is achieved.
This process creates new data points along line segments between minority feature vectors, effectively making the decision boundary of the minority class more inclusive. 
We decided to include SMOTE in our study as it is a reference method, even though important drawbacks have been detected in previous articles, 
such as presenting computational complexity quadratic in the size of the minority class and distortion of its distribution due to localisation of the selected target points.



\begin{comment}
	\begin{figure}[H]
	\label{fig:tube}
	\centering
	\includegraphics[height=4.0cm]{tube.jpg}
	\caption{Conservation in thin long tube (3D), with variables varying only in one dimension (from C.P. Fall, 2005)}
	\end{figure}
\end{comment}


\subsubsection{Borderline SMOTE}

Borderline-SMOTE is a specialized technique designed to handle imbalanced datasets. It focuses on oversampling examples near class boundaries, which are crucial for accurate classification. Unlike traditional oversampling methods that generate synthetic examples for the entire minority class, Borderline-SMOTE specifically targets these borderline examples within the minority class.

This method determines if a minority class instance is eligible for oversampling with the SMOTE technique based on the majority class representation among its nearest neighbors. By doing so, Borderline-SMOTE ensures a more precise and effective oversampling strategy.

Furthermore, implementing specific variants like Borderline-SMOTE I or Borderline-SMOTE II can be advantageous, especially in cases with outliers or sparsely scattered minority observations. These variants provide an extra layer of refinement to the oversampling process, enhancing the algorithm's performance.

In essence, Borderline-SMOTE's targeted approach, coupled with its careful consideration of instance eligibility, equips it to strengthen the learning process, particularly in regions near class boundaries where accurate classification is most critical. By incorporating this technique, imbalanced datasets can undergo a tailored oversampling treatment, ultimately improving the model's ability to navigate complex class boundaries \cite{Nguyen2009}, \cite{Gupta2018}, \cite{Brandt2020}.


%\subsubsection{K-means SMOTE}


\textbf{ADASYN} is another synthetic sampler 
The ADASYN algorithm addresses class imbalance by generating synthetic observations tailored to the learning difficulty of specific minority class instances. It focuses on producing more synthetic data for minority observations that pose a greater challenge for the model to learn. This approach is similar to SMOTE, as it also creates synthetic observations for the minority class. However, ADASYN distinguishes itself by prioritizing the generation of synthetic data for instances that are comparatively harder to learn within the given model.

It generates additional synthetic observations for minority class instances located in regions where there are more majority class observations within the k-nearest neighbors' range. Conversely, if a minority observation has no majority class neighbors within this range, no synthetic data is generated for that particular instance. The rationale behind this lies in the understanding that instances without nearby majority class samples are inherently more challenging to learn than those situated further from majority class observations.

Fundamentally, ADASYN's approach is rooted in the adaptive generation of minority data samples, tailoring the synthesis process to the distribution and learning difficulty of the minority class \cite{Brandt2020}, \cite{He2008}.









\begin{comment}
% I put this algorithm here for reference on how algorithm2e is used
\NoCaptionOfAlgo
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlCapSkip{1em}
\SetAlCapNameFnt{\normalfont\normalsize}
%\TitleOfAlgo{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}
\caption{Timestep $\textcolor{darkgray}{t \to t + \delta t}$}

Select a random sample of the minority class\;

\For{c in \texttt{cells}}{
    $
    \textcolor{darkgray}{r_c} :=
    \begin{cases*}
        \color{teal}{r_S} &\text{if c is sensitive} \\
        \color{purple}{r_R} &\text{if c is resistent}\\
    \end{cases*}$\;
    Total propensity $\textcolor{darkgray}{p := (r_c + d_T) \delta t}$\;
    \uIf{$\textcolor{Purple}{u < p}$}{ \tcp{cell active}
        \uIf{$\textcolor{Purple}{u < \frac{r_c \delta t}{p}}$}{
            \texttt{AttemptProliferation}\;
        }
        \Else{
            \texttt{Death}\;
        }
    } 
    \Else{  \tcp{cell inactive}
        \texttt{continue}\;
    }
}
\end{algorithm}
\end{comment}
