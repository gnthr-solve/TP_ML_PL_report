
\subsection{Metrics}

This section gives a brief overview of the metrics commonly used to assess the performance of a classifier in a binary classification situation.
Given the true y-values of a test set and classifiers predictions on the corresponding feature vectors most metrics are based on the so called \textbf{confusion matrix}.

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.75\linewidth]{assets/confusion_matrix.png}
  	\captionof{figure}{Confusion Matrix from wikipedia}
  	\label{fig:confusion_matrix}
\end{figure}

It summarises performance with the number of true positves (TP), true negatives (TN), false positives (FP) and false negatives (FN). 
What is a positive depends on the target class. In our case this is the class $1$ (the minority class) while a negative corresponds to class $0$ (the majority class).

From these values many standard measures are derived:\\

\textbf{Accuracy:}
Accuracy is the ratio between all that has been labeled correctly and the total number of samples, i.e.
\[
	\text{acc} = \frac{TP + TN}{TP + TN + FP + FN}
\]
It is a commonly used but fairly basic measure that often fails to accurately represent a classifiers performance in an imbalanced situation,
as for a high class imbalance the accuracy is high for a classifier that stubbornly predicts majority class for every sample.\\

\textbf{Sensitivity / True Positive Rate / Recall:}
Recall reflects the fraction of positive samples the classifier has identified i.e.
\[
	\text{rec} = \frac{TP}{TP + FN}
\]
It is more relevant in the imbalanced case as a classifier that chooses to label all before it as majority class to obtain good accuracy will have low recall.\\

\textbf{Precision:}
Precision represents the fraction of samples that have been correctly labeled positive i.e.
\[
	\text{prec} = \frac{TP}{TP + FP}
\]
This measure allows to assess whether the classifier tends to be correct when labelling a sample as minority class.\\

\textbf{False Positive Rate:}
FPR gives the fraction of negative samples that are still incorrectly classified as positive.
\[
	\text{FPR} = \frac{FP}{TN + FP}
\]
It is used to compute the receiver operator characteristic and relates to the next measure.\\

\textbf{Specificity:}
Gives the fraction of samples classified as correctly negative versus all negatives.
\[
	\text{spec} = \frac{TN}{TN + FP} = 1 - \text{FPR}
\]
Specificity is especially important when assessing the frequency and potential cost of false positives.\\


We have already described four different measures based on the confusion matrix, each of them assessing different aspects of the performance of a classifier.
But since all of these contribute important information, and since a classifier can have good scores in one but bad scores in another, 
how can one give a unified answer to the question "How good is my classifier?" when it comes to evaluation and decision making?
This question is the idea for the measures that follow.\\

One measure that intends to combine at least two of the metrics mentioned above is the \textbf{F1-score}.
It is the harmonic mean of precision and recall, where the harmonic mean for a set of positive real numbers $x_1, \dots, x_n$ is given by
\[
	H(x_1, \dotsm x_n) = \frac{n}{\sum_{j=1}^n \frac{1}{x_j}}
\]
which applied to precision and recall becomes
\[
	F = \frac{2}{ \frac{1}{\text{rec}} + \frac{1}{\text{prec}} } = 2 \frac{\text{rec} \, \text{prec}}{ \text{rec} + \text{prec} }.
\]
There are also weighted versions of the F1-score that are supposed to take into account whether recall or precision are more important in a given situation.\\

Given the test set, the classifiers we used in our project predict the probabilities of class membership for each sample.
The standard way in which they then map these probabilities to a class prediction is by simply predicting the class with the largest probability.
In practical application this approach may be flawed however. 
For a good classifier the predicted probability, which represents the confidence it has that a new sample belongs in the respective class, 
should closely match the actual probability of the class given the samples features. 
When a classifiers decision involves risk a user might want select a specific probability threshold for a positive prediction. \\

%Another important measure that provides a more comprehensive assessment of a classifiers performance is the 
\textbf{Receiver Operator Characteristic (ROC)}
ROC curves take this issue into account, by assessing the performance of a classifier across a range of probability thresholds.
On a theoretical level, supposing here $X$ is the random variable representing the distribution of the feature vectors 
and our classifier outputs a continuous probability score $s(X)$, the ROC is obtained by considering different cutoff thresholds for that score.
Suppose that if $s(X) > \tau$, we predict the positive class, then the true positive rate (TPR) and the false positive rate (FPR) are given by
\[
    	\begin{aligned}
    		TPR(\tau) &= \mathbb{P}(s(X) > \tau | Y = 1) \\
    		FPR(\tau) &= \mathbb{P}(s(X) > \tau | Y = 0)
    	\end{aligned}
\]
where $Y$ is the r.v. representing the true class of $X$ for every realisation. 
%In this theoretical context we can assign a new meaning to two measures we saw earlier:
%TPR is the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative instance. 
%Similarly, FPR is the probability that the classifier ranks a randomly chosen negative instance higher than a randomly chosen positive instance.

The ROC curve plots $TPR(\tau)$ against $FPR(\tau)$ for all possible thresholds $\tau$, producing a curve that ranges from $(0,0)$ to $(1,1)$.
We can interpret a ROC plot as plotting the path of a function
\[
	f: \mathbb{R} \to [0,1]^2, \quad  \tau \mapsto (TPR(\tau), FPR(\tau))
\]

\begin{figure}[H]
  	\centering
  	\includegraphics[width=0.5\linewidth]{assets/Roc_curve.png}
  	\captionof{figure}{Illustration of a ROC curve from wikipedia by MartinThoma}
  	\label{fig:roc_curve}
\end{figure}

The \textbf{Area Under the ROC Curve (AUC)} is, as the name suggests, the integral of the area under the ROC curve of a model
It provides a single scalar value that represents the expected performance of the classifier under a variety of cutoff thresholds.
An AUC of $1$ indicates perfect discrimination, while an AUC of $0.5$ indicates a prediction performance no better than random chance.

AUC can also be interpreted in terms of the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance,
assuming that one positive and one negative instance are chosen at random \cite{hanley1982meaning}.

In practice ROC curves and AUC values are obtained by sorting the classified samples by score thus AUC is essentially a rank order statistic.

\textbf{Calibration}
We mentioned before that a classifiers probability predictions should in a perfect world correspond to the actual class membership probability of a feature vector.
Expressed theoretically again this would mean for the probability function $p$ of the classifier that given a new feature sample $x_\text{new}$
\[
	p(x_\text{new}) = \mathbb{P}(Y = 1 | X = x_\text{new})
\]
Of course the conditional probability on the right hand side of this equation is unknown in practice and thus this condition cannot be directly evaluated.
Calibration curves make some simplifications to still have some measure of how good the probability predictions of a model are.
Instead of matching the classifiers prediction with the conditional probability a calibration curve is created by comparing mean predicted probabilities 
with mean frequency of occurrence on a test set.
This is done by firstly sorting the predicted probabilities from lowest to highest and aligning them with the corresponding true class value.
Then the aligned lists are binned and for each bin the mean predicted probability and the mean frequency of class $1$ occurrence are calculated.
One then plots, either directly or after interpolation, the predicted probability versus the mean frequency.
For details on the implementation please check the corresponding methods in the \texttt{IterMetrics} or \texttt{FMLP\_Metrics} class 
in the \href{https://github.com/gnthr-solve/TP_ML_Pipeline}{github repository} of our project.\\

If the predicted probabilities line up perfectly with the mean frequency they form a line matching the identity function.
A model that fulfils this is called well calibrated.
What seems counterintuitive at first sight is that a model can have an excellent AUC value but at the same time be poorly calibrated and vice versa.
An optimal model would have both good calibration and good discrimination, but this is difficult to achieve.
Especially for imbalanced datasets one often has to sacrifice one for the other.

\begin{comment}
\subsubsection{Decision Theoretic Measures}
Suppose the classifier is presented with mammography image data and the predicted probabilities for cancer in a case are $49\%$ for class $0$ (i.e. healthy tissue)
and $51\%$ for class $1$ (i.e. malignancy). The classifier would confidently predict a cancer diagnosis that may incur a risky biopsy for the patient.
Depending on the circumstances one might want to adapt or at least assess the probability threshold at which the classifier predicts class $1$.
To assess a classifiers performance in a context like this measures have been proposed that take into account the predicted probabilities and the risks involved.

\textbf{Risk Threshold}
Suppose r.v. $Y$ with outcome diseased $D$ or healthy $\neg D$.
Let $c_{TP}, c_{FP}, c_{TN}, c_{FN}$ then expected cost of predicting $\neg D$ is
\[
	\mathbb{P}(Y = 1) c_{FN} + \mathbb{P}(Y = 0) c_{TN}
\]

and expected cost for predicting $D$ is
\[
	\mathbb{P}(Y = 1) c_{TP} + \mathbb{P}(Y = 0) c_{FP}
\]

If we write $T = \mathbb{P}(Y = 1)$ we can rewrite to
\[
	T c_{FN} + (1-T) c_{TN}
\]
and
\[
	T c_{TP} + (1-T) c_{FP}
\]
One is indifferent about treatment if both expected costs are equal.
\[
\begin{aligned}
	T c_{FN} + (1-T) c_{TN} &= T c_{TP} + (1-T) c_{FP} \\
	&\Leftrightarrow \\
	T &= \frac{c_{TN} - c_{FP}}{(c_{TN} - c_{FP}) + (c_{TP} - c_{FN})} \\
\end{aligned}
\]
It can also be rearranged in a different way
\[
\begin{aligned}
	T c_{FN} + (1-T) c_{TN} &= T c_{TP} + (1-T) c_{FP} \\
	&\Leftrightarrow \\
	\frac{T}{1 - T} &= \frac{ c_{TN} - c_{FP} }{ c_{TP} - c_{FN} } \\
\end{aligned}
\]

In the first group, the costs relate to undertreatment (false-negative classifications)
The costs of these false-negative classifications $c_{FN}$ should be compared to the costs of true-positive classifications $c_{TP}$.
The difference $c_{TP} - c_{FN}$ is the net benefit of treating all who have the disease compared to treating none of them.
Suppose you have a treatment that causes lots of damage as well as curing the disease. 
Then this difference might be low, i.e. treating everyone with the dangerous treatment does not give much better outcome than simply not treating anyone.
Suppose on the other hand you have a devastating disease like Polio and a low cost treatment like a Polio-vaccine, then that difference will be strongly positive.

In the second group, relevant costs are for those without the event if not treated, who are treated (“overtreated”).
The costs of these false-positive classifications ($c_{FP}$) should be compared to the costs of true-negative classifications ($c_{TN}$)
while $c_{TN} - c_{FP}$ is the harm of treating all who don't have the disease compared to the benefit of not treating any of them.
E.g. the cost of not treating anyone without the disease might be 0 but the cost of treating them might be high. Then this value is strongly negative.
On the other hand suppose again a low impact vaccine that barely does harm, then that difference may be small negative. 

Odds (cutoff) = Harm/Benefit

The ratio in case of e.g. the polio vaccine could be strongly in favour of treating everyone (small cost for those without, high benefit for those with disease).


\textbf{Net Benefit}
The choice of risk threshold implicitly conveys the adopted relative misclassification costs. 
It can be derived that the odds of the risk threshold equal the harm-to-benefit ratio, which is the harm of a false positive divided by the benefit of a true positive.
For example, if a risk threshold of $20\%$ is used, the odds are 1 to 4. 
Therefore, a $20\%$ risk threshold assumes that the harm of a false positive is one-quarter of the benefit of a true positive or that 1 true positive is worth 4 false positives: 
A clinician might express this in terms such as ‘‘I would not do more than five biopsies to find one cancer.’’ 
Hence, when applying a model to a set of patients,
we can correct the number of true positives (TP) for the number of false positive (FP) using the odds $w$ of the risk threshold $t$: 
\[
	\text{TP} - w \text{FP} = \text{TP} - \frac{\tau}{1-\tau} \text{FP}
\]
When dividing by the total sample size $N$, the Net Benefit is obtained
\[
	\text{NB} = \frac{1}{N} (\text{TP} - w \text{FP}) = \frac{1}{N} (\text{TP} - \frac{\tau}{1-\tau} \text{FP})
\]

The Net Benefit of treat-none is always 0, 
whereas the Net Benefit of treat-all is positive for risk thresholds below the event rate and negative for risk thresholds above the event rate.
	
\end{comment}
	
