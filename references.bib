@article{Costa2023,
abstract = {Robust learning is an important issue in Scientific Machine Learning (SciML). There are several works in the literature addressing this topic. However, there is an increasing demand for methods that can simultaneously consider all the different uncertainty components involved in SciML model identification. Hence, this work proposes a comprehensive methodology for uncertainty evaluation of the SciML that also considers several possible sources of uncertainties involved in the identification process. The uncertainties considered in the proposed method are the absence of a theory, causal models, sensitivity to data corruption or imperfection, and computational effort. Therefore, it is possible to provide an overall strategy for uncertainty-aware models in the SciML field. The methodology is validated through a case study developing a soft sensor for a polymerization reactor. The first step is to build the nonlinear model parameter probability distribution (PDF) by Bayesian inference. The second step is to obtain the machine learning model uncertainty by Monte Carlo simulations. In the first step, a PDF with 30,000 samples is built. In the second step, the uncertainty of the machine learning model is evaluated by sampling 10,000 values through Monte Carlo simulation. The results demonstrate that the identified soft sensors are robust to uncertainties, corroborating the consistency of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {2209.01900},
author = {Costa, Erbet Almeida and Rebello, Carine de Menezes and Fontana, M{\'{a}}rcio and Schnitman, Leizer and Nogueira, Idelfonso Bessa dos Reis},
doi = {10.3390/math11010074},
eprint = {2209.01900},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A_Robust_Learning_Methodology_for_Uncertainty-awar.pdf:pdf},
issn = {22277390},
journal = {Mathematics},
keywords = {Markov Chain Monte Carlo,robust learning,scientific machine learning,uncertainty},
number = {1},
pages = {0--3},
title = {{A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models}},
volume = {11},
year = {2023}
}
@article{Schultz2022,
abstract = {Data is commonly stored in tabular format. Several fields of research are prone to small imbalanced tabular data. Supervised Machine Learning on such data is often difficult due to class imbalance. Synthetic data generation, i.e., oversampling, is a common remedy used to improve classifier performance. State-of-the-art linear interpolation approaches, such as LoRAS and ProWRAS can be used to generate synthetic samples from the convex space of the minority class to improve classifier performance in such cases. Deep generative networks are common deep learning approaches for synthetic sample generation, widely used for synthetic image generation. However, their scope on synthetic tabular data generation in the context of imbalanced classification is not adequately explored. In this article, we show that existing deep generative models perform poorly compared to linear interpolation based approaches for imbalanced classification problems on smaller tabular datasets. To overcome this, we propose a deep generative model, ConvGeN that combines the idea of convex space learning with deep generative models. ConvGeN learns the coefficients for the convex combinations of the minority class samples, such that the synthetic data is distinct enough from the majority class. Our benchmarking experiments demonstrate that our proposed model ConvGeN improves imbalanced classification on such small datasets, as compared to existing deep generative models, while being at-par with the existing linear interpolation approaches. Moreover, we discuss how our model can be used for synthetic tabular data generation in general, even outside the scope of data imbalance and thus, improves the overall applicability of convex space learning.},
archivePrefix = {arXiv},
arxivId = {2206.09812},
author = {Schultz, Kristian and Bej, Saptarshi and Hahn, Waldemar and Wolfien, Markus and Srivastava, Prashant and Wolkenhauer, Olaf},
eprint = {2206.09812},
file = {:home/zekiye/Documents/Master_lectures/Team Project/ConvGeN\: Convex space learning.pdf:pdf},
pages = {1--20},
title = {{ConvGeN: Convex space learning improves deep-generative oversampling for tabular imbalanced classification on smaller datasets}},
url = {http://arxiv.org/abs/2206.09812},
year = {2022}
}
@article{Lever2017,
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.4346},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Principal component.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
number = {7},
pages = {641--642},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Principal component analysis}},
url = {http://dx.doi.org/10.1038/nmeth.4346},
volume = {14},
year = {2017}
}
@article{Krzywinski2013,
author = {Krzywinski, Martin},
doi = {10.1038/nmeth.2337},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Axes, ticks and grids.pdf:pdf},
issn = {15487091},
journal = {Nature Methods},
number = {3},
pages = {183},
publisher = {Nature Publishing Group},
title = {{Points of view: Axes, ticks and grids}},
volume = {10},
year = {2013}
}
@article{Kaur2019,
abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
doi = {10.1145/3343440},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A Systematic Review on Imbalanced Data Challenges.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Data analysis,Data imbalance,Machine learning,Sampling},
number = {4},
title = {{A systematic review on imbalanced data challenges in machine learning: Applications and solutions}},
volume = {52},
year = {2019}
}
@article{Alahmari2020,
abstract = {Data imbalance with respect to the class labels has been recognised as a challenging problem for machine learning techniques as it has a direct impact on the classification model's performance. In an imbalanced dataset, most of the instances belong to one class, while far fewer instances are associated with the remaining classes. Most of the machine learning algorithms tend to favour the majority class and ignore the minority classes leading to classification models being generated that cannot be generalised. This paper investigates the problem of class imbalance for a medical application related to autism spectrum disorder (ASD) screening to identify the ideal data resampling method that can stabilise classification performance. To achieve the aim, experimental analyses to measure the performance of different oversampling and under-sampling techniques have been conducted on a real imbalanced ASD dataset related to adults. The results produced by multiple classifiers on the considered datasets showed superiority in terms of specificity, sensitivity, and precision, among others, when adopting oversampling techniques in the pre-processing phase.},
author = {Alahmari, Fahad},
doi = {10.1142/S021964922040016X},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A Comparison of Resampling Techniques.pdf:pdf},
issn = {02196492},
journal = {Journal of Information and Knowledge Management},
keywords = {Autism spectrum disorder,ROC,class imbalance,classification,machine learning,precision,sensitivity,specificity},
number = {1},
pages = {1--13},
title = {{A Comparison of Resampling Techniques for Medical Data Using Machine Learning}},
volume = {19},
year = {2020}
}
@article{Al.2018,
abstract = {环境中砷与铬暴露对于儿童肾功能的影响},
author = {et Al., Richards},
doi = {10.4049/jimmunol.1801473.The},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Commentary\: The Problem of Class Imbalance in Biomedical.pdf:pdf},
issn = {0000000000},
journal = {Physiology & behavior},
keywords = {criteria pollutants,ozone,particulate matter},
number = {5},
pages = {139--148},
title = {{乳鼠心肌提取 HHS Public Access}},
volume = {176},
year = {2018}
}
@article{Altman2015,
abstract = {Correlation implies association, but not causation. Conversely, causation implies association, but not correlation.},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3587},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Association, correlation.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
number = {10},
pages = {899--900},
pmid = {26688882},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Association, correlation and causation}},
url = {http://dx.doi.org/10.1038/nmeth.3587},
volume = {12},
year = {2015}
}
@article{Al.2018,
abstract = {环境中砷与铬暴露对于儿童肾功能的影响},
author = {et Al., Richards},
doi = {10.4049/jimmunol.1801473.The},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Commentary\: The Problem of Class Imbalance in Biomedical.pdf:pdf},
issn = {0000000000},
journal = {Physiology & behavior},
keywords = {criteria pollutants,ozone,particulate matter},
number = {5},
pages = {139--148},
title = {{乳鼠心肌提取 HHS Public Access}},
volume = {176},
year = {2018}
}
@article{WernerdeVargas2023,
abstract = {Machine Learning (ML) algorithms have been increasingly replacing people in several application domains—in which the majority suffer from data imbalance. In order to solve this problem, published studies implement data preprocessing techniques, cost-sensitive and ensemble learning. These solutions reduce the naturally occurring bias towards the majority sample through ML. This study uses a systematic mapping methodology to assess 9927 papers related to sampling techniques for ML in imbalanced data applications from 7 digital libraries. A filtering process selected 35 representative papers from various domains, such as health, finance, and engineering. As a result of a thorough quantitative analysis of these papers, this study proposes two taxonomies—illustrating sampling techniques and ML models. The results indicate that oversampling and classical ML are the most common preprocessing techniques and models, respectively. However, solutions with neural networks and ensemble ML models have the best performance—with potentially better results through hybrid sampling techniques. Finally, none of the 35 works apply simulation-based synthetic oversampling, indicating a path for future preprocessing solutions.},
author = {{Werner de Vargas}, Vitor and {Schneider Aranda}, Jorge Arthur and {dos Santos Costa}, Ricardo and {da Silva Pereira}, Paulo Ricardo and {Vict{\'{o}}ria Barbosa}, Jorge Luis},
doi = {10.1007/s10115-022-01772-8},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Imbalanced data preprocessing techniques for machine.pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Imbalanced data,Machine learning,Preprocessing techniques,Sampling,Systematic mapping study},
number = {1},
pages = {31--57},
publisher = {Springer London},
title = {{Imbalanced data preprocessing techniques for machine learning: a systematic mapping study}},
url = {https://doi.org/10.1007/s10115-022-01772-8},
volume = {65},
year = {2023}
}
@article{Chawla2004,
abstract = {methods for bal- ancing machine training . digital text categorization from docu- ments Toward scalable with non-uniform class},
author = {Chawla, Nitesh V and Japkowicz, Nathalie and Kotcz, Aleksander},
doi = {10.1145/1007730.1007733},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Editorial\: Special Issue on Learning from Imbalanced Data.pdf:pdf},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {1--6},
title = {{Editorial}},
volume = {6},
year = {2004}
}
@article{Demidova2017,
abstract = {In this paper a new approach to selection of the optimal parameters values for the SMOTE (Synthetic Minority Over-sampling Technique) algorithm in the problem of the SVM (Support Vector Machine) classification of imbalanced datasets has been suggested. This approach allows reducing the time expenditures for the search of the optimum parameters values of the SMOTE algorithm. The experimental results show that the offered approach allows increasing the classification quality of the SVM classifier.},
author = {Demidova, Liliya and Klyueva, Irina},
doi = {10.1109/MECO.2017.7977136},
file = {:home/zekiye/Documents/Master_lectures/Team Project/SVM_classification_Optimization_with_the_SMOTE_algorithm_for_the_class_imbalance_problem.pdf:pdf},
isbn = {9781509067411},
journal = {2017 6th Mediterranean Conference on Embedded Computing, MECO 2017 - Including ECYPS 2017, Proceedings},
keywords = {SVM classifier,hybridization,imbalanced dataset,particle swarm optimization algorithm,sampling},
number = {June},
pages = {1--4},
publisher = {IEEE},
title = {{SVM classification: Optimization with the SMOTE algorithm for the class imbalance problem}},
year = {2017}
}
@article{Tarawneh2022,
abstract = {For the last two decades, oversampling has been employed to overcome the challenge of learning from imbalanced datasets. Many approaches to solving this challenge have been offered in the literature. Oversampling, on the other hand, is a concern. That is, models trained on fictitious data may fail spectacularly when put to real-world problems. The fundamental difficulty with oversampling approaches is that, given a real-life population, the synthesized samples may not truly belong to the minority class. As a result, training a classifier on these samples while pretending they represent minority may result in incorrect predictions when the model is used in the real world. We analyzed a large number of oversampling methods in this paper and devised a new oversampling evaluation system based on hiding a number of majority examples and comparing them to those generated by the oversampling process. Based on our evaluation system, we ranked all these methods based on their incorrectly generated examples for comparison. Our experiments using more than 70 oversampling methods and nine imbalanced real-world datasets reveal that all oversampling methods studied generate minority samples that are most likely to be majority. Given data and methods in hand, we argue that oversampling in its current forms and methodologies is unreliable for learning from class imbalanced data and should be avoided in real-world applications.},
author = {Tarawneh, Ahmad S. and Hassanat, Ahmad B. and Altarawneh, Ghada Awad and Almuhaimeed, Abdullah},
doi = {10.1109/ACCESS.2022.3169512},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Stop_Oversampling_for_Class_Imbalance_Learning_A_Review-2.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Hassanat metric,Oversampling,SMOTE,imbalanced datasets,machine learning},
pages = {47643--47660},
publisher = {IEEE},
title = {{Stop Oversampling for Class Imbalance Learning: A Review}},
volume = {10},
year = {2022}
}
@article{Liu2022,
abstract = {Background: Imbalance between positive and negative outcomes, a so-called class imbalance, is a problem generally found in medical data. Despite various studies, class imbalance has always been a difficult issue. The main objective of this study was to find an effective integrated approach to address the problems posed by class imbalance and to validate the method in an early screening model for a rare cardiovascular disease aortic dissection (AD). Methods: Different data-level methods, cost-sensitive learning, and the bagging method were combined to solve the problem of low sensitivity caused by the imbalance of two classes of data. First, feature selection was applied to select the most relevant features using statistical analysis, including significance test and logistic regression. Then, we assigned two different misclassification cost values for two classes, constructed weak classifiers based on the support vector machine (SVM) model, and integrated the weak classifiers with undersampling and bagging methods to build the final strong classifier. Due to the rarity of AD, the data imbalance was particularly prominent. Therefore, we applied our method to the construction of an early screening model for AD disease. Clinical data of 523,213 patients from the Institute of Hypertension, Xiangya Hospital, Central South University were used to verify the validity of this method. In these data, the sample ratio of AD patients to non-AD patients was 1:65, and each sample contained 71 features. Results: The proposed ensemble model achieved the highest sensitivity of 82.8%, with training time and specificity reaching 56.4 s and 71.9% respectively. Additionally, it obtained a small variance of sensitivity of 19.58 × 10–3 in the seven-fold cross validation experiment. The results outperformed the common ensemble algorithms of AdaBoost, EasyEnsemble, and Random Forest (RF) as well as the single machine learning (ML) methods of logistic regression, decision tree, k nearest neighbors (KNN), back propagation neural network (BP) and SVM. Among the five single ML algorithms, the SVM model after cost-sensitive learning method performed best with a sensitivity of 79.5% and a specificity of 73.4%. Conclusions: In this study, we demonstrate that the integration of feature selection, undersampling, cost-sensitive learning and bagging methods can overcome the challenge of class imbalance in a medical dataset and develop a practical screening model for AD, which could lead to a decision support for screening for AD at an early stage.},
author = {Liu, Lijue and Wu, Xiaoyu and Li, Shihao and Li, Yi and Tan, Shiyang and Bai, Yongping},
doi = {10.1186/s12911-022-01821-w},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Solving the class imbalance problem using.pdf:pdf},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Aortic dissection,Class imbalance,Ensemble learning,SVM},
number = {1},
pages = {1--16},
pmid = {35346181},
title = {{Solving the class imbalance problem using ensemble algorithm: application of screening for aortic dissection}},
volume = {22},
year = {2022}
}
@article{WernerdeVargas2023,
abstract = {Machine Learning (ML) algorithms have been increasingly replacing people in several application domains—in which the majority suffer from data imbalance. In order to solve this problem, published studies implement data preprocessing techniques, cost-sensitive and ensemble learning. These solutions reduce the naturally occurring bias towards the majority sample through ML. This study uses a systematic mapping methodology to assess 9927 papers related to sampling techniques for ML in imbalanced data applications from 7 digital libraries. A filtering process selected 35 representative papers from various domains, such as health, finance, and engineering. As a result of a thorough quantitative analysis of these papers, this study proposes two taxonomies—illustrating sampling techniques and ML models. The results indicate that oversampling and classical ML are the most common preprocessing techniques and models, respectively. However, solutions with neural networks and ensemble ML models have the best performance—with potentially better results through hybrid sampling techniques. Finally, none of the 35 works apply simulation-based synthetic oversampling, indicating a path for future preprocessing solutions.},
author = {{Werner de Vargas}, Vitor and {Schneider Aranda}, Jorge Arthur and {dos Santos Costa}, Ricardo and {da Silva Pereira}, Paulo Ricardo and {Vict{\'{o}}ria Barbosa}, Jorge Luis},
doi = {10.1007/s10115-022-01772-8},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Imbalanced data preprocessing techniques for machine.pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Imbalanced data,Machine learning,Preprocessing techniques,Sampling,Systematic mapping study},
number = {1},
pages = {31--57},
publisher = {Springer London},
title = {{Imbalanced data preprocessing techniques for machine learning: a systematic mapping study}},
url = {https://doi.org/10.1007/s10115-022-01772-8},
volume = {65},
year = {2023}
}
@article{Nusinovici2020,
abstract = {Objective: To evaluate the performance of machine learning (ML) algorithms and to compare them with logistic regression for the prediction of risk of cardiovascular diseases (CVDs), chronic kidney disease (CKD), diabetes (DM), and hypertension (HTN) and in a prospective cohort study using simple clinical predictors. Study Design and Setting: We conducted analyses in a population-based cohort study in Asian adults (n = 6,762). Five different ML models were considered—single-hidden-layer neural network, support vector machine, random forest, gradient boosting machine, and k-nearest neighbor—and were compared with standard logistic regression. Results: The incidences at 6 years of CVD, CKD, DM, and HTN cases were 4.0%, 7.0%, 9.2%, and 34.6%, respectively. Logistic regression reached the highest area under the receiver operating characteristic curve for CKD (0.905 [0.88, 0.93]) and DM (0.768 [0.73, 0.81]) predictions. For CVD and HTN, the best models were neural network (0.753 [0.70, 0.81]) and support vector machine (0.780 [0.747, 0.812]), respectively. However, the differences with logistic regression were small (less than 1%) and nonsignificant. Logistic regression, gradient boosting machine, and neural network were systematically ranked among the best models. Conclusion: Logistic regression yields as good performance as ML models to predict the risk of major chronic diseases with low incidence and simple clinical predictors.},
author = {Nusinovici, Simon and Tham, Yih Chung and {Chak Yan}, Marco Yu and {Wei Ting}, Daniel Shu and Li, Jialiang and Sabanayagam, Charumathi and Wong, Tien Yin and Cheng, Ching Yu},
doi = {10.1016/j.jclinepi.2020.03.002},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Logistic regression was as good as machine learning.pdf:pdf},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Chronic diseases,Interaction,Logistic regression,Machine learning,Nonlinearity,Prognostic modeling},
pages = {56--69},
pmid = {32169597},
publisher = {Elsevier Inc.},
title = {{Logistic regression was as good as machine learning for predicting major chronic diseases}},
url = {https://doi.org/10.1016/j.jclinepi.2020.03.002},
volume = {122},
year = {2020}
}
@article{Khushi2021,
abstract = {Medical datasets are usually imbalanced, where negative cases severely outnumber positive cases. Therefore, it is essential to deal with this data skew problem when training machine learning algorithms. This study uses two representative lung cancer datasets, PLCO and NLST, with imbalance ratios (the proportion of samples in the majority class to those in the minority class) of 24.7 and 25.0, respectively, to predict lung cancer incidence. This research uses the performance of 23 class imbalance methods (resampling and hybrid systems) with three classical classifiers (logistic regression, random forest, and LinearSVC) to identify the best imbalance techniques suitable for medical datasets. Resampling includes ten under-sampling methods (RUS, etc.), seven over-sampling methods (SMOTE, etc.), and two integrated sampling methods (SMOTEENN, SMOTE-Tomek). Hybrid systems include (Balanced Bagging, etc.). The results show that class imbalance learning can improve the classification ability of the model. Compared with other imbalanced techniques, under-sampling techniques have the highest standard deviation (SD), and over-sampling techniques have the lowest SD. Over-sampling is a stable method, and the AUC in the model is generally higher than in other ways. Using ROS, the random forest performs the best predictive ability and is more suitable for the lung cancer datasets used in this study. The code is available at https://mkhushi.github.io/},
author = {Khushi, Matloob and Shaukat, Kamran and Alam, Talha Mahboob and Hameed, Ibrahim A. and Uddin, Shahadat and Luo, Suhuai and Yang, Xiaoyan and Reyes, Maranatha Consuelo},
doi = {10.1109/ACCESS.2021.3102399},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A_Comparative_Performance_Analysis_of_Data_Resampling_Methods_on_Imbalance_Medical_Data.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Class imbalance,data resampling,healthcare,lung cancer,machine learning},
pages = {109960--109975},
title = {{A Comparative Performance Analysis of Data Resampling Methods on Imbalance Medical Data}},
volume = {9},
year = {2021}
}
@article{Mienye2021,
abstract = {Many real-world machine learning applications require building models using highly imbalanced datasets. Usually, in medical datasets, the healthy patients or samples are dominant, making them the majority class, while the sick patients are few, making them the minority class. Researchers have proposed numerous machine learning methods to predict medical diagnosis. Still, the class imbalance problem makes it difficult for classifiers to adequately learn and distinguish between the minority and majority classes. Cost-sensitive learning and resampling techniques are used to deal with the class imbalance problem. This research focuses on developing robust cost-sensitive classifiers by modifying the objective functions of some well-known algorithms, such as logistic regression, decision tree, extreme gradient boosting, and random forest, which are then used to efficiently predict medical diagnosis. Meanwhile, as opposed to resampling techniques, our approach does not alter the original data distribution. Firstly, we implement the standard versions of these algorithms to provide a baseline for performance comparison. Secondly, we develop their corresponding cost-sensitive algorithms. For the proposed approaches, it is not necessary to change the distribution of the original data as the modified algorithms consider the imbalanced class distribution during training, thereby resulting in more reliable performance than when the data is resampled. Four popular medical datasets, including the Pima Indians Diabetes, Haberman Breast Cancer, Cervical Cancer Risk Factors, and Chronic Kidney Disease datasets, are used in the experiments to validate the performance of the proposed approach. The experimental results show that the cost-sensitive methods yield superior performance compared to the standard algorithms.},
author = {Mienye, Ibomoiye Domor and Sun, Yanxia},
doi = {10.1016/j.imu.2021.100690},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Performance analysis of cost-sensitive learning methods_with_application toimbalanced medical data:},
issn = {23529148},
journal = {Informatics in Medicine Unlocked},
keywords = {Cost-sensitive learning,Imbalanced classification,Machine learning,Medical diagnosis},
pages = {100690},
publisher = {Elsevier Ltd},
title = {{Performance analysis of cost-sensitive learning methods with application to imbalanced medical data}},
url = {https://doi.org/10.1016/j.imu.2021.100690},
volume = {25},
year = {2021}
}
@article{Radivojac2004,
abstract = {We consider the problem of classification in noisy, high-dimensional, and class-imbalanced protein datasets. In order to design a complete classification system, we use a three-stage machine learning framework consisting of a feature selection stage, a method addressing noise and class-imbalance, and a method for combining biologically related tasks through a prior-knowledge based clustering. In the first stage, we employ Fisher's permutation test as a feature selection filter. Comparisons with the alternative criteria show that it may be favorable for typical protein datasets. In the second stage, noise and class imbalance are addressed by using minority class over-sampling, majority class under-sampling, and ensemble learning. The performance of logistic regression models, decision trees, and neural networks is systematically evaluated. The experimental results show that in many cases ensembles of logistic regression classifiers may outperform more expressive models due to their robustness to noise and low sample density in a high-dimensional feature space. However, ensembles of neural networks may be the best solution for large datasets. In the third stage, we use prior knowledge to partition unlabeled data such that the class distributions among non-overlapping clusters significantly differ. In our experiments, training classifiers specialized to the class distributions of each cluster resulted in a further decrease in classification error. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Radivojac, Predrag and Chawla, Nitesh V. and Dunker, A. Keith and Obradovic, Zoran},
doi = {10.1016/j.jbi.2004.07.008},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Classification and knowledge discovery in protein databases.pdf:pdf},
isbn = {0000000000},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Class imbalance,Class-distribution estimation,Classification,Clustering,Feature selection,Noise},
number = {4},
pages = {224--239},
pmid = {15465476},
title = {{Classification and knowledge discovery in protein databases}},
volume = {37},
year = {2004}
}
@article{VanDenGoorbergh2022,
abstract = {Objective: Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models. Material and Methods: Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: No correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented. Results: The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: The probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead. Discussion: Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed. Conclusion: Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.},
archivePrefix = {arXiv},
arxivId = {2202.09101},
author = {{Van Den Goorbergh}, Ruben and {Van Smeden}, Maarten and Timmerman, Dirk and {Ben Van Calster}},
doi = {10.1093/jamia/ocac093},
eprint = {2202.09101},
file = {:home/zekiye/Documents/Master_lectures/Team Project/The harm of class imbalance corrections for risk.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {calibration,class imbalance,logistic regression,synthetic minority oversampling technique,undersampling},
number = {9},
pages = {1525--1534},
pmid = {35686364},
title = {{The harm of class imbalance corrections for risk prediction models: Illustration and simulation using logistic regression}},
volume = {29},
year = {2022}
}
@article{Brandt2020,
abstract = {In this thesis, the performance of two over-sampling techniques, SMOTE and ADASYN, is compared. The comparison is done on three imbalanced data sets using three different classification models and evaluation metrics, while varying the way the data is pre-processed. The results show that both SMOTE and ADASYN improve the performance of the classifiers in most cases. It is also found that SVM in conjunction with SMOTE performs better than with ADASYN as the degree of class imbalance increases. Furthermore, both SMOTE and ADASYN increase the relative performance of the Random forest as the degree of class imbalance grows. However, no pre-processing method consistently outperforms the other in its contribution to better performance as the degree of class imbalance varies.},
author = {Brandt, Jakob and Lanz{\'{e}}n, Emil},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A Comparative Review of SMOTE.pdf:pdf},
keywords = {ADASYN,F-measure,Machine learning,Matthews correlation coefficient 3,SMOTE,Sensitivity,class imbalance,classification,over-sampling,supervised learning},
pages = {42},
title = {{A Comparative Review of SMOTE and ADASYN in Imbalanced Data Classification}},
year = {2020}
}
@article{Nguyen2009,
author = {Nguyen, Hien M and Cooper, Eric W and Kamei, Katsuari},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Borderline Over-sampling for Imbalanced Data.pdf:pdf},
pages = {24--29},
title = {{Borderline Over-sampling for Imbalanced Data Classification}},
year = {2009}
}
@article{Jin2020,
abstract = {In human resource management, employee turnover problem is heavily concerned by managers since the leave of key employees can bring great loss to the company. However, most existing researches are employee-centered, which ignored the historical events of turnover behaviors or the longitudinal data of job records. In this paper, from an event-centered perspective, we design a hybrid model based on survival analysis and machine learning, and propose a turnover prediction algorithm named RFRSF, which combines survival analysis for censored data processing and ensemble learning for turnover behavior prediction. In addition, we take strategies to handle employees with multiple turnover records so as to construct survival data with censored records. We compare RFRSF with several baseline methods on a real dataset crawled from one of the biggest online professional social platforms of China. The results show that the survival analysis model can significantly benefit the employee turnover prediction performance.},
author = {Jin, Ziwei and Shang, Jiaxing and Zhu, Qianwen and Ling, Chen and Xie, Wu and Qiang, Baohua},
doi = {10.1007/978-3-030-62008-0_35},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Random forest.pdf:pdf},
isbn = {9783030620073},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Machine learning,Professional social networks,Random survival forests,Survival analysis,Turnover prediction},
pages = {503--515},
title = {{RFRSF: Employee Turnover Prediction Based on Random Forests and Survival Analysis}},
volume = {12343 LNCS},
year = {2020}
}
@article{OBrien2019,
abstract = {Extending previous work on quantile classifiers (q-classifiers) we propose the q*-classifier for the class imbalance problem. The classifier assigns a sample to the minority class if the minority class conditional probability exceeds 0 < q* < 1, where q* equals the unconditional probability of observing a minority class sample. The motivation for q*-classification stems from a density-based approach and leads to the useful property that the q*-classifier maximizes the sum of the true positive and true negative rates. Moreover, because the procedure can be equivalently expressed as a cost-weighted Bayes classifier, it also minimizes weighted risk. Because of this dual optimization, the q*-classifier can achieve near zero risk in imbalance problems, while simultaneously optimizing true positive and true negative rates. We use random forests to apply q*-classification. This new method which we call RFQ is shown to outperform or is competitive with existing techniques with respect to G-mean performance and variable selection. Extensions to the multiclass imbalanced setting are also considered.},
author = {O'Brien, Robert and Ishwaran, Hemant},
doi = {10.1016/j.patcog.2019.01.036},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A random forests quantile classifier for class imbalanced data.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Class imbalance,Minority class,Random forests,Response-based sampling,Weighted Bayes classifier},
pages = {232--249},
publisher = {Elsevier Ltd},
title = {{A random forests quantile classifier for class imbalanced data}},
url = {https://doi.org/10.1016/j.patcog.2019.01.036},
volume = {90},
year = {2019}
}
@article{Wang2019,
abstract = {Motivation: The prediction of protein-protein interaction (PPI) sites is a key to mutation design, catalytic reaction and the reconstruction of PPI networks. It is a challenging task considering the significant abundant sequences and the imbalance issue in samples. Results: A new ensemble learning-based method, Ensemble Learning of synthetic minority oversampling technique (SMOTE) for Unbalancing samples and RF algorithm (EL-SMURF), was proposed for PPI sites prediction in this study. The sequence profile feature and the residue evolution rates were combined for feature extraction of neighboring residues using a sliding window, and the SMOTE was applied to oversample interface residues in the feature space for the imbalance problem. The Multi-dimensional Scaling feature selection method was implemented to reduce feature redundancy and subset selection. Finally, the Random Forest classifiers were applied to build the ensemble learning model, and the optimal feature vectors were inserted into EL-SMURF to predict PPI sites. The performance validation of EL-SMURF on two independent validation datasets showed 77.1% and 77.7% accuracy, which were 6.2-15.7% and 6.1-18.9% higher than the other existing tools, respectively.},
author = {Wang, Xiaoying and Yu, Bin and Ma, Anjun and Chen, Cheng and Liu, Bingqiang and Ma, Qin},
doi = {10.1093/bioinformatics/bty995},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Protein–protein interaction sites prediction.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {14},
pages = {2395--2402},
pmid = {30520961},
title = {{Protein-protein interaction sites prediction by ensemble random forests with synthetic minority oversampling technique}},
volume = {35},
year = {2019}
}
@article{Xu2020,
abstract = {The problem of imbalanced data classification often exists in medical diagnosis. Traditional classification algorithms usually assume that the number of samples in each class is similar and their misclassification cost during training is equal. However, the misclassification cost of patient samples is higher than that of healthy person samples. Therefore, how to increase the identification of patients without affecting the classification of healthy individuals is an urgent problem. In order to solve the problem of imbalanced data classification in medical diagnosis, we propose a hybrid sampling algorithm called RFMSE, which combines the Misclassification-oriented Synthetic minority over-sampling technique (M-SMOTE) and Edited nearset neighbor (ENN) based on Random forest (RF). The algorithm is mainly composed of three parts. First, M-SMOTE is used to increase the number of samples in the minority class, while the over-sampling rate of M-SMOTE is the misclassification rate of RF. Then, ENN is used to remove the noise ones from the majority samples. Finally, RF is used to perform classification prediction for the samples after hybrid sampling, and the stopping criterion for iterations is determined according to the changes of the classification index (i.e. Matthews Correlation Coefficient (MCC)). When the value of MCC continuously drops, the process of iterations will be stopped. Extensive experiments conducted on ten UCI datasets demonstrate that RFMSE can effectively solve the problem of imbalanced data classification. Compared with traditional algorithms, our method can improve F-value and MCC more effectively.},
author = {Xu, Zhaozhao and Shen, Derong and Nie, Tiezheng and Kou, Yue},
doi = {10.1016/j.jbi.2020.103465},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A hybrid sampling algorithm combining M-SMOTE and ENN based on.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Data resampling,Imbalanced data classification,Medical diagnosis,Random forest},
number = {May 2019},
pages = {103465},
pmid = {32512209},
publisher = {Elsevier},
title = {{A hybrid sampling algorithm combining M-SMOTE and ENN based on Random forest for medical imbalanced data}},
url = {https://doi.org/10.1016/j.jbi.2020.103465},
volume = {107},
year = {2020}
}
@article{Fotouhi2019,
abstract = {The early diagnosis of cancer, as one of the major causes of death, is vital for cancerous patients. Diagnosing diseases in general and cancer in particular is a considerable application of data analysis for medical science. However, imbalanced data distribution and imbalanced quality of the majority and minority classes, which lead to misclassification, is a great challenge in this field. Though the samples of the majority class and their proper classification are more important to classifier, cancer is diagnosed by relying on the minority class samples (cancer data class). While the consequence of wrong diagnosis for non-cancerous patients is several additional clinical tests, the cancerous patients pay the price of wrong diagnosis with their lives. As such, studying the class imbalance problem is vital from the medical's perspective. To serve this purpose, a comprehensive study on the consequences of imbalanced data problem is performed in this paper on the data of cancer patients for the first time. In this context, oversampling and under sampling as two main balancing techniques including 18 algorithms are employed. The techniques used in oversampling are ADASYN, ADOMS, AHC, Borderline-SMOTE, ROS, Safe-Level-SMOTE, SMOTE, SMOTE-ENN, SMOTE-TL, SPIDER and SPIDER2, while under sampling techniques are CNN, CNNTL, NCL, OSS, RUS, SBC and TL. To examine the impact of balancers on the performance of classifiers, four classifiers named RIPPER, MLP, KNN, and C4.5 are employed as learners. In addition, 15 cancer data sets from SEER program used for the study are kidney, soft tissue, bladder, rectum, colon, bone, larynx, breast, cervix, prostate, oropharynx, melanoma, thyroid, testis, and lip. The findings of the study are centered on examining the impact of class imbalance on the function of classifiers, a general comparing of the function of pre-processing techniques and classifying all data sets and finally determining the best balancer and classifier for each kind of cancer data set. According to the results, significant improvement is obtained through using balancers. Assessing by AUC, the performance of different classifiers of cancer imbalanced data sets has improved in 90% of the cases after using balancing techniques. To be more precise, Friedman statistical tests are applied and interestingly, each kind of cancer data set responded differently to different balancing techniques and classifiers. Moreover, considering the mean rank of each technique and classifier that were used for data sets, oversampling balancing techniques result in better outcomes than under sampling ones.},
author = {Fotouhi, Sara and Asadi, Shahrokh and Kattan, Michael W.},
doi = {10.1016/j.jbi.2018.12.003},
file = {:home/zekiye/Documents/Master_lectures/Team Project/1-s2.0-S1532046418302302-main.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Classification,Data pre-processing,Diagnosis of cancer,Imbalanced data},
number = {December 2018},
pages = {103089},
pmid = {30611011},
publisher = {Elsevier},
title = {{A comprehensive data level analysis for cancer diagnosis on imbalanced data}},
url = {https://doi.org/10.1016/j.jbi.2018.12.003},
volume = {90},
year = {2019}
}
@article{He2008,
abstract = {This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics. {\textcopyright}2008 IEEE.},
author = {He, Haibo and Bai, Yang and Garcia, Edwardo A. and Li, Shutao},
doi = {10.1109/IJCNN.2008.4633969},
file = {:home/zekiye/Documents/Master_lectures/Team Project/He, Garcia, Li - ADASYN_ Adaptive Synthetic Sampling Approach for Imbalanced Learning (2008) - 2008-He-ieee 2.pdf:pdf},
isbn = {9781424418213},
journal = {Proceedings of the International Joint Conference on Neural Networks},
number = {3},
pages = {1322--1328},
title = {{ADASYN: Adaptive synthetic sampling approach for imbalanced learning}},
year = {2008}
}
@article{Gupta2018,
abstract = {The functioning of electromyogram (EMG) driven prosthesis to control the performance of artificial prosthetic arms placed on people with missing limbs depends on the cumulative effect of multiple dynamic factors, some of which include electrode placement position, muscle contraction levels, forearm orientations, etc. However, the study of the combined influence of these dynamic factors has been limited and hence offered us scope to improve the accuracy of the previous studies. We used the data to extract multiple features through the Time Dependent Power Spectrum Descriptor (TD-PSD) algorithm, which has proven to be one of the best methods of feature extraction. Samples are classified using the Neural Pattern Recognition Toolbox with scaled conjugate gradient backpropagation as the training algorithm, which gives an improved accuracy over Support Vector Machine (SVM) classifier. Neural Network is trained using the EMG signals of 10 subjects performing multiple hand movements to achieve classification accuracy up to 94.7%. The results obtained are a testimony to the fact that the suggested method is competent to improve the operation of pattern recognition myoelectric signals.},
author = {Gupta, Tanmay and Yadav, Jyoti and Chaudhary, Shubham and Agarwal, Utkarsh},
doi = {10.1007/978-3-319-68385-0_20},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Borderline-SMOTE\: A New Over-Sampling Method in.pdf:pdf},
isbn = {9783319683843},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Classification,Clustering,Feature extraction,Pattern recognition},
pages = {232--242},
title = {{EMG pattern classification using neural networks}},
volume = {683},
year = {2018}
}
@article{Le2019,
abstract = {The diagnosis of bankruptcy companies becomes extremely important for business owners, banks, governments, securities investors, and economic stakeholders to optimize the profitability as well as to minimize risks of investments. Many studies have been developed for bankruptcy prediction utilizing different machine learning approaches on various datasets around the world. Due to the class imbalance problem occurring in the bankruptcy datasets, several special techniques would be used to improve the prediction performance. Oversampling technique and cost-sensitive learning framework are two common methods for dealing with class imbalance problem. Using oversampling techniques and cost-sensitive learning framework independently also improves predictability. However, for datasets with very small balancing ratios, combining two above techniques will produce the better results. Therefore, this study develops a hybrid approach using oversampling technique and cost-sensitive learning, namely, HAOC for bankruptcy prediction on the Korean Bankruptcy dataset. The first module of HAOC is oversampling module with an optimal balancing ratio found in the first experiment that will give the best overall performance for the validation set. Then, the second module uses the cost-sensitive learning model, namely, CBoost algorithm to bankruptcy prediction. The experimental results show that HAOC will give the best performance value for bankruptcy prediction compared with the existing approaches.},
author = {Le, Tuong and Vo, Minh Thanh and Vo, Bay and Lee, Mi Young and Baik, Sung Wook},
doi = {10.1155/2019/8460934},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A Hybrid Approach Using Oversampling Technique and.pdf:pdf},
issn = {10990526},
journal = {Complexity},
title = {{A Hybrid Approach Using Oversampling Technique and Cost-Sensitive Learning for Bankruptcy Prediction}},
volume = {2019},
year = {2019}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from unbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy. {\textcopyright} 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:home/zekiye/Documents/Master_lectures/Team Project/SMOTE\: Synthetic Minority Over-sampling Technique.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
title = {{SMOTE: Synthetic minority over-sampling technique}},
volume = {16},
year = {2002}
}
@article{Krawczyk2016,
abstract = {Despite more than two decades of continuous development learning from imbalanced data is still a focus of intense research. Starting as a problem of skewed distributions of binary tasks, this topic evolved way beyond this conception. With the expansion of machine learning and data mining, combined with the arrival of big data era, we have gained a deeper insight into the nature of imbalanced learning, while at the same time facing new emerging challenges. Data-level and algorithm-level methods are constantly being improved and hybrid approaches gain increasing popularity. Recent trends focus on analyzing not only the disproportion between classes, but also other difficulties embedded in the nature of data. New real-life problems motivate researchers to focus on computationally efficient, adaptive and real-time methods. This paper aims at discussing open issues and challenges that need to be addressed to further develop the field of imbalanced learning. Seven vital areas of research in this topic are identified, covering the full spectrum of learning from imbalanced data: classification, regression, clustering, data streams, big data analytics and applications, e.g., in social media and computer vision. This paper provides a discussion and suggestions concerning lines of future research for each of them.},
author = {Krawczyk, Bartosz},
doi = {10.1007/s13748-016-0094-0},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Learning from imbalanced data\: open challenges.pdf:pdf},
issn = {21926360},
journal = {Progress in Artificial Intelligence},
keywords = {Big data,Data streams,Imbalanced clustering,Imbalanced data,Imbalanced regression,Machine learning,Multi-class imbalance},
number = {4},
pages = {221--232},
publisher = {Springer Berlin Heidelberg},
title = {{Learning from imbalanced data: open challenges and future directions}},
volume = {5},
year = {2016}
}
@article{Bej2021,
abstract = {The Synthetic Minority Oversampling TEchnique (SMOTE) is widely-used for the analysis of imbalanced datasets. It is known that SMOTE frequently over-generalizes the minority class, leading to misclassifications for the majority class, and effecting the overall balance of the model. In this article, we present an approach that overcomes this limitation of SMOTE, employing Localized Random Affine Shadowsampling (LoRAS) to oversample from an approximated data manifold of the minority class. We benchmarked our algorithm with 14 publicly available imbalanced datasets using three different Machine Learning (ML) algorithms and compared the performance of LoRAS, SMOTE and several SMOTE extensions that share the concept of using convex combinations of minority class data points for oversampling with LoRAS. We observed that LoRAS, on average generates better ML models in terms of F1-Score and Balanced accuracy. Another key observation is that while most of the extensions of SMOTE we have tested, improve the F1-Score with respect to SMOTE on an average, they compromise on the Balanced accuracy of a classification model. LoRAS on the contrary, improves both F1 Score and the Balanced accuracy thus produces better classification models. Moreover, to explain the success of the algorithm, we have constructed a mathematical framework to prove that LoRAS oversampling technique provides a better estimate for the mean of the underlying local data distribution of the minority class data space.},
archivePrefix = {arXiv},
arxivId = {1908.08346},
author = {Bej, Saptarshi and Davtyan, Narek and Wolfien, Markus and Nassar, Mariam and Wolkenhauer, Olaf},
doi = {10.1007/s10994-020-05913-4},
eprint = {1908.08346},
file = {:home/zekiye/Documents/Master_lectures/Team Project/LoRAS\: an oversampling approach for imbalanced datasets.pdf:pdf},
isbn = {0123456789},
issn = {15730565},
journal = {Machine Learning},
keywords = {Data augmentation,Imbalanced datasets,Manifold learning,Oversampling,Synthetic sample generation},
number = {2},
pages = {279--301},
publisher = {Springer US},
title = {{LoRAS: an oversampling approach for imbalanced datasets}},
url = {https://doi.org/10.1007/s10994-020-05913-4},
volume = {110},
year = {2021}
}
@article{Rus,
author = {Rus, R O S},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Pipeline_Mindmap.pdf:pdf},
title = {{Data Generation Balancing Algorithms Classification Methods Generator Class}}
}
@article{Chawla2004,
abstract = {methods for bal- ancing machine training . digital text categorization from docu- ments Toward scalable with non-uniform class},
author = {Chawla, Nitesh V. and Japkowicz, Nathalie and Kotcz, Aleksander},
doi = {10.1145/1007730.1007733},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Editorial\: Special Issue on Learning from Imbalanced Data.pdf:pdf},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {1--6},
title = {{Editorial}},
volume = {6},
year = {2004}
}
@article{Viloria2020,
abstract = {Nowadays, the DL algorithms show good results when used in the solution of different problems which present similar characteristics as the great amount of data and high dimensionality. However, one of the main challenges that currently arises is the classification of high dimensionality databases, with very few samples and high-class imbalance. Biomedical databases of gene expression microarrays present the characteristics mentioned above, presenting problems of class imbalance, with few samples and high dimensionality. The problem of class imbalance arises when the set of samples belonging to one class is much larger than the set of samples of the other class or classes. This problem has been identified as one of the main challenges of the algorithms applied in the context of Big Data. The objective of this research is the study of genetic expression databases, using conventional methods of sub and oversampling for the balance of classes such as RUS, ROS and SMOTE. The databases were modified by applying an increase in their imbalance and in another case generating artificial noise.},
author = {Viloria, Amelec and Lezama, Omar Bonerge Pineda and Mercado-Caruzo, Nohora},
doi = {10.1016/j.procs.2020.07.018},
file = {:home/zekiye/Documents/Master_lectures/Team Project/Unbalanced data processing using oversampling.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Deep learning techniques,Genetic expression,Imbalance of classes,Microarray databases},
pages = {108--113},
publisher = {Elsevier B.V.},
title = {{Unbalanced data processing using oversampling: Machine learning}},
url = {https://doi.org/10.1016/j.procs.2020.07.018},
volume = {175},
year = {2020}
}
@article{Tarawneh2020,
abstract = {Class imbalance occurs in classification problems in which the 'normal' cases, or instances, significantly outnumber the 'abnormal' instances. Training a standard classifier on imbalanced data leads to predictive biases which cause poor performance on the class(es) with lower prior probabilities. The less frequent classes are often critically important events, such as system failure or the occurrence of a rare disease. As a result, the class imbalance problem has been considered to be of great importance for many years. In this paper, we propose a novel algorithm that utilizes the furthest neighbor of a candidate example to generate new synthetic samples. A key advantage of SOMTEFUNA over existing methods is that it does not have parameters to tune (such as K in SMOTE). Thus, it is significantly easier to utilize in real-world applications. We evaluate the benefit of resampling with SOMTEFUNA against state-of-the-art methods including SMOTE, ADASYN and SWIM using Naive Bayes and Support Vector Machine classifiers. Also, we provide a statistical analysis based on Wilcoxon Signed-rank test to validate the significance of the SMOTEFUNA results. The results indicate that the proposed method is an efficient alternative to the current methods. Specifically, SOMTEFUNA achieves better 5-fold cross validated ROC and precision-recall space performance.},
author = {Tarawneh, Ahmad S. and Hassanat, Ahmad B.A. and Almohammadi, Khalid and Chetverikov, Dmitry and Bellinger, Colin},
doi = {10.1109/ACCESS.2020.2983003},
file = {:home/zekiye/Documents/Master_lectures/Team Project/SMOTEFUNA.pdf:pdf},
isbn = {1620170000},
issn = {21693536},
journal = {IEEE Access},
keywords = {Binary classification,SMOTE,data mining algorithm,furthest neighbor,imbalance problem},
number = {April},
pages = {59069--59082},
title = {{SMOTEFUNA: Synthetic Minority Over-Sampling Technique Based on Furthest Neighbour Algorithm}},
volume = {8},
year = {2020}
}
@article{Saadatfar2020,
abstract = {The K-nearest neighbors (KNN) machine learning algorithm is a well-known nonparametric classification method. However, like other traditional data mining methods, applying it on big data comes with computational challenges. Indeed, KNN determines the class of a new sample based on the class of its nearest neighbors; however, identifying the neighbors in a large amount of data imposes a large computational cost so that it is no longer applicable by a single computing machine. One of the proposed techniques to make classification methods applicable on large datasets is pruning. LC-KNN is an improved KNN method which first clusters the data into some smaller partitions using the K-means clustering method; and then applies the KNN for each new sample on the partition which its center is the nearest one. However, because the clusters have different shapes and densities, selection of the appropriate cluster is a challenge. In this paper, an approach has been proposed to improve the pruning phase of the LC-KNN method by taking into account these factors. The proposed approach helps to choose a more appropriate cluster of data for looking for the neighbors, thus, increasing the classification accuracy. The performance of the proposed approach is evaluated on different real datasets. The experimental results show the effectiveness of the proposed approach and its higher classification accuracy and lower time cost in comparison to other recent relevant methods.},
author = {Saadatfar, Hamid and Khosravi, Samiyeh and Joloudari, Javad Hassannataj and Mosavi, Amir and Shamshirband, Shahaboddin},
doi = {10.3390/math8020286},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A New K-Nearest Neighbors Classifier for Big Data.pdf:pdf},
issn = {22277390},
journal = {Mathematics},
keywords = {Artificial intelligence,Big data,Classification,Classifier,Cluster density,Cluster shape,Clustering,Computation,Data science,K-nearest neighbors,KNN,Machine learning,Machine learning for big data,Reinforcement learning},
number = {2},
pages = {1--12},
title = {{A new k-nearest neighbors classifier for big data based on efficient data pruning}},
volume = {8},
year = {2020}
}
@article{Akogul2016,
abstract = {Clustering analysis based on a mixture of multivariate normal distributions is commonly used in the clustering of multidimensional data sets. Model selection is one of the most important problems in mixture cluster analysis based on the mixture of multivariate normal distributions. Model selection involves the determination of the number of components (clusters) and the selection of an appropriate covariance structure in the mixture cluster analysis. In this study, the efficiency of information criteria that are commonly used in model selection is examined. The effectiveness of information criteria has been determined according to the success in the selection of the number of components and in the selection of an appropriate covariance matrix.},
author = {Akogul, Serkan and Erisoglu, Murat},
doi = {10.3390/mca21030034},
file = {:home/zekiye/Documents/Master_lectures/Team Project/A Comparison of Information Criteria in Clustering Based on Mixture of Multivariate Normal Distributions.pdf:pdf},
issn = {22978747},
journal = {Mathematical and Computational Applications},
keywords = {Cluster Analysis,Information Criteria,Mixture Models},
number = {3},
title = {{A comparison of information criteria in clustering based on mixture of multivariate normal distributions}},
volume = {21},
year = {2016}
}
@book{Sablatnig2005,
author = {Sablatnig, Robert and Kropatsch, Walter and Hanbury, Allan},
booktitle = {Lecture Notes in Computer Science},
file = {:home/zekiye/Documents/Master_lectures/Team Project/978-3-642-13278-0.pdf:pdf},
isbn = {9783642132773},
issn = {03029743},
title = {{Lecture Notes in Computer Science: Preface}},
volume = {3663},
year = {2005}
}
